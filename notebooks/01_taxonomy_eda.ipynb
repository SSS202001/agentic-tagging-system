{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b99e9601",
   "metadata": {},
   "source": [
    "# Phase 1: Taxonomy Discovery & EDA\n",
    "\n",
    "**Objective:** Analyze the raw proposal data to derive a compact, evidence-based taxonomy.\n",
    "\n",
    "This notebook represents the \"Research Phase.\" We use Llama 3.3 to read the entire dataset and generate a Mutually Exclusive, Collectively Exhaustive (MECE) set of categories.\n",
    "\n",
    "**Output:** A `taxonomy.json` file saved to the `data/` directory, which will be used by the production agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87f81efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ API Key Loaded.\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 1. SETUP & CONFIGURATION\n",
    "# ==========================================\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 1. Load Environment Variables (from root .env)\n",
    "# We assume this notebook is in 'notebooks/', so .env is one level up\n",
    "load_dotenv(os.path.join('..', '.env'))\n",
    "\n",
    "api_key = os.environ.get(\"GROQ_API_KEY\")\n",
    "if not api_key:\n",
    "    print(\"‚ö†Ô∏è  Warning: GROQ_API_KEY not found. Please check your .env file.\")\n",
    "else:\n",
    "    print(\"‚úÖ API Key Loaded.\")\n",
    "\n",
    "# 2. Configure Paths\n",
    "DATA_DIR = os.path.join('..', 'data')\n",
    "INPUT_FILE = os.path.join(DATA_DIR, 'proposals.csv')\n",
    "OUTPUT_TAXONOMY = os.path.join(DATA_DIR, 'taxonomy.json')\n",
    "\n",
    "# 3. Initialize Client\n",
    "client = Groq(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b40567a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loaded 66 proposals.\n",
      "\n",
      "Sample Data:\n",
      "  proposalId                                        description\n",
      "0       AL-1  for constructing the Bridge Replacement (Gradi...\n",
      "1       AK-1  This federally funded contract includes all ne...\n",
      "2       AZ-1  The proposed project is located in Coconino Co...\n"
     ]
    }
   ],
   "source": [
    "# --- DATA INGESTION & VALIDATION ---\n",
    "\n",
    "# 1. Fail Fast Check\n",
    "if not os.path.exists(INPUT_FILE):\n",
    "    print(f\"‚ùå Error: Input file not found at {INPUT_FILE}\")\n",
    "else:\n",
    "    # 2. Load Data\n",
    "    df = pd.read_csv(INPUT_FILE)\n",
    "    print(f\"üìÇ Loaded {len(df)} proposals.\")\n",
    "    print(\"\\nSample Data:\")\n",
    "    print(df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2324bb69",
   "metadata": {},
   "source": [
    "### Step 1: Generate Taxonomy\n",
    "We feed all descriptions to Llama 3.3-70B to find natural clusters. We use a **system prompt** that enforces a strict JSON output schema to ensure the results are machine-readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "876be383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Analyzing dataset to derive taxonomy...\n",
      "\n",
      "‚úÖ Taxonomy saved to ..\\data\\taxonomy.json\n",
      "   Categories: ['Bridge Construction and Repair', 'Roadway Rehabilitation and Resurfacing', 'Traffic and Pedestrian Infrastructure', 'Maintenance and Upkeep', 'Drainage and Water Management', 'Intelligent Transportation Systems (ITS) and Technology', 'Landscaping and Environmental', 'Utility and Facility Upgrades']\n"
     ]
    }
   ],
   "source": [
    "def generate_taxonomy(dataframe):\n",
    "    \"\"\"\n",
    "    Phase 1 Core Logic: Taxonomy Discovery.\n",
    "    Uses an LLM to read the entire dataset context and derive a schema.\n",
    "    \"\"\"\n",
    "    print(\"üöÄ Analyzing dataset to derive taxonomy...\")\n",
    "\n",
    "    \n",
    "    all_descriptions = \"\\n\".join([\n",
    "        f\"- {row['description']}\"\n",
    "        for _, row in dataframe.iterrows()\n",
    "    ])\n",
    "\n",
    "    # --- PROMPT ENGINEERING ---\n",
    "    # 1. Persona: \"Senior Data Architect\" primes the model for structure.\n",
    "    # 2. Constraint: \"Mutually Exclusive\" prevents overlapping tags.\n",
    "    # 3. Output Control: \"JSON only\" ensures we can parse it programmatically.\n",
    "    prompt = f\"\"\"\n",
    "    You are a Senior Data Architect.\n",
    "    Analyze the following project proposals and create a strict classification taxonomy.\n",
    "\n",
    "    DATA:\n",
    "    {all_descriptions}\n",
    "\n",
    "    REQUIREMENTS:\n",
    "    1. Create 4-8 categories that cover 90% of the data.\n",
    "    2. Categories must be Mutually Exclusive.\n",
    "    3. Output JSON only.\n",
    "\n",
    "    OUTPUT SCHEMA:\n",
    "    {{\n",
    "      \"taxonomy\": {{\n",
    "        \"Category Name\": {{\n",
    "          \"definition\": \"Strict 1-sentence definition.\",\n",
    "          \"keywords\": [\"key\", \"words\"],\n",
    "          \"text_example\": \"A generic example phrase based on the data (DO NOT use specific IDs)\"\n",
    "        }}\n",
    "      }}\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    # --- LLM INFERENCE ---\n",
    "    # Using JSON mode and Temperature 0.0 for deterministic, valid JSON output.\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"llama-3.3-70b-versatile\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.0,\n",
    "        response_format={\"type\": \"json_object\"}\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        data = json.loads(completion.choices[0].message.content)\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå JSON Parsing Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- EXECUTION ---\n",
    "# Run the generation and persist the artifact for Phase 2 (the Agent)\n",
    "taxonomy_data = generate_taxonomy(df)\n",
    "\n",
    "if taxonomy_data:\n",
    "    # Save to disk so the 'main.py' pipeline can load it later.\n",
    "    with open(OUTPUT_TAXONOMY, 'w') as f:\n",
    "        json.dump(taxonomy_data, f, indent=2)\n",
    "    print(f\"\\n‚úÖ Taxonomy saved to {OUTPUT_TAXONOMY}\")\n",
    "    print(f\"   Categories: {list(taxonomy_data['taxonomy'].keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7971cfc7",
   "metadata": {},
   "source": [
    "### Step 2: Validate Coverage (Spot Check)\n",
    "We test the new taxonomy against a random sample to ensure the definitions are clear and the model can actually use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9de8fd9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Spot Checking 5 random proposals...\n",
      "   AZ-1: Roadway Rehabilitation and Resurfacing\n",
      "   ND-1: Roadway Rehabilitation and Resurfacing\n",
      "   LA-2: Unknown\n",
      "   FL-2: Landscaping and Environmental\n",
      "   NS-1: Maintenance and Upkeep\n"
     ]
    }
   ],
   "source": [
    "def validate_sample(dataframe, taxonomy):\n",
    "    print(\"\\nüîç Spot Checking 5 random proposals...\")\n",
    "    \n",
    "    # --- 1. RANDOM SAMPLING ---\n",
    "    sample = dataframe.sample(5)\n",
    "    sample_text = \"\\n\".join([f\"ID {row['proposalId']}: {row['description']}\" for _, row in sample.iterrows()])\n",
    "    \n",
    "    # --- 2. VALIDATION PROMPT ---\n",
    "    prompt = f\"\"\"\n",
    "    Taxonomy:\n",
    "    {json.dumps(taxonomy['taxonomy'], indent=2)}\n",
    "\n",
    "    Task: Classify these proposals.\n",
    "    \n",
    "    Proposals:\n",
    "    {sample_text}\n",
    "\n",
    "    Return JSON: {{ \"results\": [ {{ \"id\": \"...\", \"category\": \"...\" }} ] }}\n",
    "    \"\"\"\n",
    "\n",
    "    # --- 3. TEST INFERENCE ---\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"llama-3.3-70b-versatile\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.0,\n",
    "        response_format={\"type\": \"json_object\"}\n",
    "    )\n",
    "    \n",
    "    # --- 4. VISUAL INSPECTION ---\n",
    "    results = json.loads(completion.choices[0].message.content)\n",
    "    for res in results.get('results', []):\n",
    "        print(f\"   {res['id']}: {res['category']}\")\n",
    "\n",
    "# Execute Validation\n",
    "if taxonomy_data:\n",
    "    validate_sample(df, taxonomy_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
