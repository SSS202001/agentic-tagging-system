# Agentic RFP Tagging System

A scalable, self-correcting classification pipeline designed to automate the tagging of Request for Proposal (RFP) documents.

![System Architecture](images/architecture.png)

## ðŸ“– Project Overview
This system addresses the challenge of categorizing unstructured proposal data into a strict taxonomy. Unlike traditional classifiers that struggle with "tag drift" or hallucinations, this project implements a two-phase **Agentic Workflow**:

1.  **Taxonomy Discovery:** Dynamically derives a MECE (Mutually Exclusive, Collectively Exhaustive) taxonomy from the global dataset context.
2.  **Deterministic Inference:** Uses a state machine (LangGraph) to apply tags, validating every output against the taxonomy before finalization.

**Key Capabilities:**
- **Automated Taxonomy Generation:** Eliminates the need for manual label creation by clustering topics from raw text.
- **Self-Correction:** Automatically detects invalid tags or hallucinations and retries the classification step.
- **Evidence Extraction:** Requires the model to provide direct text quotes supporting every tag it assigns.
- **Composite Confidence Scoring:** Calculates a composite reliability score (0.0 - 1.0) based on validation checks, evidence length, and reasoning quality.

---

## ðŸ—ï¸ Architecture & Technical Approach

### Design Pattern: The "Think-Then-Act" Workflow
The system is architected to separate the *definition* of rules from the *application* of rules.

* **Phase 1: Knowledge Engineering (Notebook)**
    * *Goal:* Establish ground truth.
    * *Method:* We process the dataset in batch to identify high-level clusters. This ensures tags are consistent across the entire corpus rather than being invented row-by-row.
    * *Outcome:* A version-controlled `taxonomy.json` artifact.

* **Phase 2: Agentic Pipeline (Production Script)**
    * *Goal:* Reliable, scalable inference.
    * *Method:* A `LangGraph` state machine orchestrates the tagging. This allows for cyclic logicâ€”if the `Validator` node detects an error, the flow routes back to the `Tagger` for a correction attempt.

### Decision Logic: Publish vs. Hold
To ensure high data quality, the system enforces a strict quality gate. Proposals are only marked **PUBLISH** if they meet a composite score threshold (`>= 0.65`).

| Component | Weight | Criteria |
| :--- | :--- | :--- |
| **Validation Pass** | `+0.50` | **Deterministic Check:** The tag MUST exist in `taxonomy.json`. (0% Hallucination Tolerance). |
| **Evidence Found** | `+0.30` | **Grounding:** The model extracted a direct quote (>10 chars) from the source text to support its decision. |
| **Reasoning** | `+0.20` | **Explainability:** The model provided a logical explanation (>15 chars) for *why* the tag applies. |

---

## ðŸ§  Architecture & Design Rationale

### Why this Architecture?
We chose a **Two-Phase Agentic Workflow** to address specific limitations observed in standard LLM pipelines:

1.  **Solving "Tag Drift" (Contextual Consistency)**
    * *The Engineering Problem:* If we classify proposals one by one (Zero-Shot), the LLM often invents slightly different tags for the same concept (e.g., "Civil Works" vs. "Construction") depending on the phrasing of the specific row.
    * *Our Solution:* **Phase 1 (Taxonomy Discovery)** analyzes the *global* dataset first to establish a "Ground Truth" schema. The agent then strictly enforces this schema during inference, ensuring 100% consistency.

2.  **Reliability via State Machines (LangGraph)**
    * *The Engineering Problem:* Simple linear chains (Prompt A â†’ Prompt B) are brittle. If the model output is malformed (e.g., missing JSON brackets), the pipeline crashes.
    * *Our Solution:* We treat the tagging process as a **Cyclic State Machine**. If the `Validator` node detects a syntax error or hallucinated tag, it doesn't crashâ€”it routes the flow back to the `Tagger` with error feedback for a second attempt (Self-Correction).

3.  **Auditability vs. Black Box**
    * *The Engineering Problem:* End-users often mistrust AI decisions and cannot verify *why* a tag was applied.
    * *Our Solution:* The **Composite Scoring System** requires the model to extract *verbatim evidence* (quotes) from the text. We prioritize explainability over raw speed, ensuring every tag is backed by source text.

---

## ðŸ“‚ Repository Structure

```text
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ proposals.csv        # Input dataset
â”‚   â”œâ”€â”€ taxonomy.json        # Derived taxonomy (Generated by Phase 1)
â”‚   â””â”€â”€ tagged_results.csv   # Final output (Generated by Phase 2)
â”‚
â”œâ”€â”€ images/
â”‚   â””â”€â”€ architecture.png     # System architecture diagram
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ 01_taxonomy_eda.ipynb  # Research notebook for taxonomy discovery
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ graph.py             # Core logic: Nodes, Edges, and State definitions
â”‚   â”œâ”€â”€ schemas.py           # Configuration, file paths, and Pydantic models
â”‚   â””â”€â”€ main.py              # Application entry point
â”‚
â”œâ”€â”€ .env.example             # API Key configuration template
â””â”€â”€ requirements.txt         # Python dependencies


ðŸš€ Setup & Installation
1. Prerequisites
Python 3.10+

A Groq Cloud API Key (Required for Llama 3.3 70B inference)

2. Install Dependencies
Clone the repository and install the required packages:

Bash

pip install -r requirements.txt
3. Configure Secrets
The system requires an API key to access the LLM.

Rename .env.example to .env.

Open the file and paste your Groq API key:

Ini, TOML

GROQ_API_KEY=gsk_your_key_here
(Note: The .env file is git-ignored to prevent secrets from leaking into version control.)

âš¡ Usage Guide
Step 1: Generate Taxonomy (Research Phase)
Before tagging, the system needs to understand the data. Run the EDA notebook to analyze the CSV and generate the taxonomy file.

File: notebooks/01_taxonomy_eda.ipynb

Action: Open in Jupyter and run all cells.

Output: Creates data/taxonomy.json.

Step 2: Run Tagging Pipeline (Production Phase)
Once the taxonomy exists, run the main pipeline script. This will load the data and process each proposal through the agentic graph.

Command:

Bash

python -m src.main
Action: The script will initialize the graph, validate inputs, and process rows sequentially with rate limiting.

Output: Creates data/tagged_results.csv.

ðŸ“Š Limitations
Scaling Considerations
For datasets exceeding 10,000 records, the following optimizations are recommended:

Context Management: Replace batch taxonomy generation with embedding-based clustering (HDBSCAN) to handle larger corpora.

Concurrency: Deploy the graph logic as an async microservice to process records in parallel.